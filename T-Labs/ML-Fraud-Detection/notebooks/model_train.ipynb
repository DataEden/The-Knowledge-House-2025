{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5bd66ee",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this notebook, we will ask you a series of questions regarding model selection. Based on your responses, we will ask you to create the ML models that you've chosen. \n",
    "\n",
    "The bonus step is completely optional, but if you provide a sufficient third machine learning model in this project, we will add `1000` points to your Kahoot leaderboard score.\n",
    "\n",
    "**Note**: Use the dataset that you've created in your previous data transformation step (not the original model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641dd43",
   "metadata": {},
   "source": [
    "## AI/ML Model Training, Testing, and Evaluation Steps/workflow:\n",
    "1. Import the necessary libraries\n",
    "2. Load data\n",
    "3. Perform EDA\n",
    "4. Preprocess data\n",
    "5. Train the model\n",
    "   - train/test/split\n",
    "   - Search\n",
    "   - Make prediction , and then \n",
    "6. Evaluate the model\n",
    "\n",
    "**More Granular:**\n",
    "- ‚òëÔ∏è Import necessary Python libraries and or modules\n",
    "- ‚òëÔ∏è Load data\n",
    "- ‚òëÔ∏è Split train/test (keep fraud ratio with stratify)\n",
    "- ‚òëÔ∏è Balance minority fraud class with SMOTE\n",
    "- ‚òëÔ∏è Scale features for all models\n",
    "- ‚òëÔ∏è Define classifiers in a dictionary \n",
    "    -  Not neccessary for model trainging, but worth doing for a more streamlined approach.\n",
    "- ‚òëÔ∏è Loop: search for best parameters using RandomizedSearchCV\n",
    "- ‚òëÔ∏è Loop: train, predict, evaluate\n",
    "- ‚òëÔ∏è Print Confusion Matrix & Classification Report for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9894067c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏èThere are currently 8 CPU-Cores available for model trainingüí™\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries and modules\n",
    "# Panda for handliing dataframes and numpy for handling arrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "# Load models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Preprocessing SMOTE Oversampling and Scaling module \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Max num n_jobs/processor(s) that's available for used\n",
    "from joblib import cpu_count\n",
    "print(f\"‚úîÔ∏èThere are currently {cpu_count()} CPU-Cores available for model trainingüí™\")\n",
    "\n",
    "# Load evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    " \n",
    "# Import modules for data manipulation\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9a55e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2:  Load data as pandas dataframe \n",
    "# Baseline transactions \n",
    "df_baseline = pd.read_csv(\"../data/baseline_transacts.csv\")\n",
    "\n",
    "# Baseline dummy encoding transform\n",
    "df_base_dummy_encode = pd.read_csv(\"../data/baseline_dummy_encode_transacts.csv\")\n",
    "\n",
    "# Log feature transform\n",
    "df_log_transacts = pd.read_csv(\"../data/log_transacts.csv\")\n",
    "\n",
    "# Log and dummy encoding transforms\n",
    "df_log_dummy_encode_transacts = pd.read_csv(\"../data/log_dummy_encode_transacts.csv\")\n",
    "\n",
    "\n",
    "# Log, change in transaction, and dummy encoding transforms\n",
    "df_log_diff_dummy_encode = pd.read_csv(\"../data/log_diff_dummy_transacts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c63bcfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud</th>\n",
       "      <th>CASH_OUT</th>\n",
       "      <th>DEBIT</th>\n",
       "      <th>PAYMENT</th>\n",
       "      <th>TRANSFER</th>\n",
       "      <th>amount_log</th>\n",
       "      <th>oldbalanceOrig_log</th>\n",
       "      <th>newbalanceOrig_log</th>\n",
       "      <th>oldbalanceDest_log</th>\n",
       "      <th>newbalanceDest_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>10.608846</td>\n",
       "      <td>11.522173</td>\n",
       "      <td>11.859486</td>\n",
       "      <td>12.400281</td>\n",
       "      <td>12.515597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12.766230</td>\n",
       "      <td>8.528529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.457159</td>\n",
       "      <td>12.779584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>7.707894</td>\n",
       "      <td>7.855409</td>\n",
       "      <td>5.871554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12.043679</td>\n",
       "      <td>14.555915</td>\n",
       "      <td>14.633881</td>\n",
       "      <td>14.796901</td>\n",
       "      <td>14.866930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12.473167</td>\n",
       "      <td>14.611048</td>\n",
       "      <td>14.722504</td>\n",
       "      <td>13.109396</td>\n",
       "      <td>12.355899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>12.774484</td>\n",
       "      <td>12.774484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>12.799063</td>\n",
       "      <td>12.799063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.848402</td>\n",
       "      <td>13.517183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>12.862446</td>\n",
       "      <td>12.862446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>13.689961</td>\n",
       "      <td>13.689961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.012266</td>\n",
       "      <td>13.699217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>10.102874</td>\n",
       "      <td>10.102874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.262824</td>\n",
       "      <td>14.278311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       isFraud  CASH_OUT  DEBIT  PAYMENT  TRANSFER  amount_log  \\\n",
       "0            0     False  False    False     False   10.608846   \n",
       "1            0      True  False    False     False   12.766230   \n",
       "2            0     False  False     True     False    7.707894   \n",
       "3            0     False  False    False     False   12.043679   \n",
       "4            0     False  False    False     False   12.473167   \n",
       "...        ...       ...    ...      ...       ...         ...   \n",
       "99995        1     False  False    False      True   12.774484   \n",
       "99996        1      True  False    False     False   12.799063   \n",
       "99997        1     False  False    False      True   12.862446   \n",
       "99998        1      True  False    False     False   13.689961   \n",
       "99999        1      True  False    False     False   10.102874   \n",
       "\n",
       "       oldbalanceOrig_log  newbalanceOrig_log  oldbalanceDest_log  \\\n",
       "0               11.522173           11.859486           12.400281   \n",
       "1                8.528529            0.000000            8.457159   \n",
       "2                7.855409            5.871554            0.000000   \n",
       "3               14.555915           14.633881           14.796901   \n",
       "4               14.611048           14.722504           13.109396   \n",
       "...                   ...                 ...                 ...   \n",
       "99995           12.774484            0.000000            0.000000   \n",
       "99996           12.799063            0.000000           12.848402   \n",
       "99997           12.862446            0.000000            0.000000   \n",
       "99998           13.689961            0.000000            9.012266   \n",
       "99999           10.102874            0.000000           14.262824   \n",
       "\n",
       "       newbalanceDest_log  \n",
       "0               12.515597  \n",
       "1               12.779584  \n",
       "2                0.000000  \n",
       "3               14.866930  \n",
       "4               12.355899  \n",
       "...                   ...  \n",
       "99995            0.000000  \n",
       "99996           13.517183  \n",
       "99997            0.000000  \n",
       "99998           13.699217  \n",
       "99999           14.278311  \n",
       "\n",
       "[100000 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first five rows\n",
    "df_log_dummy_encode_transacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe47df31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isFraud               130\n",
       "CASH_OUT              130\n",
       "DEBIT                 130\n",
       "PAYMENT               130\n",
       "TRANSFER              130\n",
       "amount_log            130\n",
       "oldbalanceOrig_log    130\n",
       "newbalanceOrig_log    130\n",
       "oldbalanceDest_log    130\n",
       "newbalanceDest_log    130\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log_dummy_encode_transacts[df_log_dummy_encode_transacts['isFraud']==1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b90a0",
   "metadata": {},
   "source": [
    "## Questions\n",
    "Is this a classification or regression task?  \n",
    "\n",
    "This is a classification task since we are effectively assigning one of two cases to either respective 'bin' of fraud and not-fraud (i.e., 0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bfb9f",
   "metadata": {},
   "source": [
    "Are you predicting for multiple classes or binary classes?  \n",
    "\n",
    "Since this is a binary classification problem, where we are trying to find out if transactions are either fraud or not-fraud, there are only 2 possible classes/labelings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd9378",
   "metadata": {},
   "source": [
    "Given these observations, which 2 (or possibly 3) machine learning models will you choose?  \n",
    "\n",
    "I'm using Logistic Regression, Support Vector Machine (SVM), and AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c408b67",
   "metadata": {},
   "source": [
    "## First Model\n",
    "\n",
    "Using the first model that you've chosen, implement the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fab3d0",
   "metadata": {},
   "source": [
    "### 1) Create a train-test split\n",
    "\n",
    "Use your cleaned and transformed dataset to divide your features and labels into training and testing sets. Make sure you‚Äôre only using numeric or properly encoded features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfdf1e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training without SMOTE and using log normalization on predictors\n",
    "# Step 3.1: Separate features (X) and target (y)\n",
    "X = df_log_dummy_encode_transacts.drop(columns=['isFraud'])  # All features except target\n",
    "y = df_log_dummy_encode_transacts['isFraud']                 # Target variable\n",
    "\n",
    "# Step 3.2: Split into training and testing sets\n",
    "# Use stratify=y to keep the fraud/non-fraud ratio consistent, but I think could be reduntdant when using SMOTE??\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e6ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data pre-processing complete: features scaled.\n",
      " [[-0.73565498 -0.08327825 -0.71638167 ...  1.38401411  0.77906622\n",
      "   0.69476195]\n",
      " [ 1.35933287 -0.08327825 -0.71638167 ... -0.84587112  1.29694534\n",
      "   1.22357173]\n",
      " [-0.73565498 -0.08327825 -0.71638167 ... -0.84587112  1.20959949\n",
      "   1.1653813 ]\n",
      " ...\n",
      " [-0.73565498 -0.08327825  1.39590395 ... -0.84587112 -1.14592334\n",
      "  -1.2493916 ]\n",
      " [ 1.35933287 -0.08327825 -0.71638167 ... -0.84587112  0.66654768\n",
      "   0.65842582]\n",
      " [-0.73565498 -0.08327825 -0.71638167 ...  1.08971234  1.05868003\n",
      "   0.96875927]]\n",
      "\n",
      "‚úÖ Data pre-processing complete: test data scaled.\n",
      " [[-0.73565498 -0.08327825 -0.71638167 ...  1.6789918   0.67758919\n",
      "   0.57115765]\n",
      " [-0.73565498 -0.08327825 -0.71638167 ... -0.84587112  1.07706241\n",
      "   1.00858466]\n",
      " [ 1.35933287 -0.08327825 -0.71638167 ... -0.84587112 -1.14592334\n",
      "   0.62668509]\n",
      " ...\n",
      " [-0.73565498 -0.08327825  1.39590395 ... -0.84587112 -1.14592334\n",
      "  -1.2493916 ]\n",
      " [ 1.35933287 -0.08327825 -0.71638167 ... -0.84587112  1.01134023\n",
      "   0.96304469]\n",
      " [ 1.35933287 -0.08327825 -0.71638167 ...  0.5755781   0.65714158\n",
      "   0.58359485]]\n"
     ]
    }
   ],
   "source": [
    "# without using SMOTE\n",
    "# Scale trianing and testing data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test) # Apply same scaler to test set\n",
    "\n",
    "# Print output from training\n",
    "print(\"‚úÖ Data pre-processing complete: features scaled.\\n\", X_train_scaled)\n",
    "print(\"\\n‚úÖ Data pre-processing complete: test data scaled.\\n\", X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0add1166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: hyperparameter Search Grids\n",
    "# Without SMOTE \n",
    "param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [100, 200, 400],\n",
    "        \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"max_depth\": range(10, 55, 5),\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"min_samples_split\": [2, 5, 20]\n",
    "    },\n",
    "    \"AdaBoost\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.5, 1.0],\n",
    "        \"estimator__max_depth\": [1, 2, 3],              # ‚úÖ Controls weak learner complexity\n",
    "        \"estimator__min_samples_split\": [2, 5, 10],     # ‚úÖ Regularization\n",
    "        \"estimator__min_samples_leaf\": [1, 2, 4]\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"C\": np.logspace(-3, 3, 7),\n",
    "        \"penalty\": [\"l1\", \"l2\"],\n",
    "        \"solver\": [\"saga\"],                      \n",
    "        \"max_iter\": [500, 1000, 1500],                       \n",
    "    },\n",
    "    \"K-Nearest Neighbors\": {\n",
    "        \"n_neighbors\": [3, 5, 7, 11],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"p\": [1, 2],\n",
    "        \"leaf_size\": [20, 30, 50],\n",
    "        \"algorithm\": [\"auto\"]\n",
    "    }\n",
    "} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a06fe1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Training and tuning: Random Forest\n",
      "‚úÖ Best Parameters for Random Forest:\n",
      " {'n_estimators': 400, 'min_samples_split': 5, 'max_features': 'log2', 'max_depth': 35, 'criterion': 'gini'}\n",
      "üìä Confusion Matrix:\n",
      " [[19973     1]\n",
      " [    8    18]]\n",
      "\n",
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000     19974\n",
      "           1      0.947     0.692     0.800        26\n",
      "\n",
      "    accuracy                          1.000     20000\n",
      "   macro avg      0.973     0.846     0.900     20000\n",
      "weighted avg      1.000     1.000     1.000     20000\n",
      "\n",
      "\n",
      "üî• Training and tuning: AdaBoost\n",
      "‚úÖ Best Parameters for AdaBoost:\n",
      " {'n_estimators': 50, 'learning_rate': 0.1, 'estimator__min_samples_split': 2, 'estimator__min_samples_leaf': 1, 'estimator__max_depth': 3}\n",
      "üìä Confusion Matrix:\n",
      " [[19973     1]\n",
      " [    9    17]]\n",
      "\n",
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000     19974\n",
      "           1      0.944     0.654     0.773        26\n",
      "\n",
      "    accuracy                          1.000     20000\n",
      "   macro avg      0.972     0.827     0.886     20000\n",
      "weighted avg      0.999     1.000     0.999     20000\n",
      "\n",
      "\n",
      "üî• Training and tuning: Logistic Regression\n",
      "‚úÖ Best Parameters for Logistic Regression:\n",
      " {'solver': 'saga', 'penalty': 'l1', 'max_iter': 1000, 'C': np.float64(10.0)}\n",
      "üìä Confusion Matrix:\n",
      " [[19973     1]\n",
      " [   12    14]]\n",
      "\n",
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      0.999     1.000     1.000     19974\n",
      "           1      0.933     0.538     0.683        26\n",
      "\n",
      "    accuracy                          0.999     20000\n",
      "   macro avg      0.966     0.769     0.841     20000\n",
      "weighted avg      0.999     0.999     0.999     20000\n",
      "\n",
      "\n",
      "üî• Training and tuning: K-Nearest Neighbors\n",
      "‚úÖ Best Parameters for K-Nearest Neighbors:\n",
      " {'weights': 'distance', 'p': 1, 'n_neighbors': 7, 'leaf_size': 30, 'algorithm': 'auto'}\n",
      "üìä Confusion Matrix:\n",
      " [[19972     2]\n",
      " [    9    17]]\n",
      "\n",
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     1.000     1.000     19974\n",
      "           1      0.895     0.654     0.756        26\n",
      "\n",
      "    accuracy                          0.999     20000\n",
      "   macro avg      0.947     0.827     0.878     20000\n",
      "weighted avg      0.999     0.999     0.999     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Define classifiers for training\n",
    "# Without SMOTE\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(solver='saga', random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier()  \n",
    "}\n",
    "\n",
    "# Store best models \n",
    "best_models = {}\n",
    "\n",
    "# Select models for training and tuning\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüî• Training and tuning: {name}\")\n",
    "    \n",
    "    param_grid = param_grids[name] # Select which param to use in randomized search\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_grid,\n",
    "        #n_iter=10,      # Tuning param\n",
    "        cv=5,\n",
    "        scoring='f1',    # Scoring param, maybe I can also try 'roc_auc'??\n",
    "        random_state=42,\n",
    "        n_jobs=-1        # Number of CPU-Cores param \n",
    "    )\n",
    "    \n",
    "    # Fit to training data\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Save the best model\n",
    "    best_models[name] = random_search.best_estimator_\n",
    "\n",
    "    # Predict on test set using the best model\n",
    "    yhat = best_models[name].predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    cm = confusion_matrix(y_test, yhat)\n",
    "    report = classification_report(y_test, yhat, digits=3)\n",
    "    \n",
    "    print(f\"‚úÖ Best Parameters for {name}:\\n\", random_search.best_params_)\n",
    "    print(\"üìä Confusion Matrix:\\n\", cm)\n",
    "    print(\"\\nüìã Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ed1bf7",
   "metadata": {},
   "source": [
    "\n",
    "### Output of Evaluation Metrics and Best Parameters of Log-Normalized Dataset using Standard Scalar, and Wtihout Using SMOTE\n",
    "\n",
    "üî• Training and tuning: Random Forest\n",
    "‚úÖ Best Parameters for Random Forest:\n",
    " {'n_estimators': 100, 'min_samples_split': 2, 'max_features': 'log2', 'max_depth': 25, 'criterion': 'entropy'}\n",
    "üìä Confusion Matrix:\n",
    " [[19973     1]\n",
    " [   10    16]]\n",
    "\n",
    "üìã Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0      0.999     1.000     1.000     19974\n",
    "           1      0.941     0.615     0.744        26\n",
    "\n",
    "    accuracy                          0.999     20000\n",
    "   macro avg      0.970     0.808     0.872     20000\n",
    "weighted avg      0.999     0.999     0.999     20000\n",
    "\n",
    "---\n",
    "\n",
    "üî• Training and tuning: AdaBoost\n",
    "‚úÖ Best Parameters for AdaBoost:\n",
    " {'n_estimators': 50, 'learning_rate': 0.5, 'estimator__min_samples_split': 5, 'estimator__min_samples_leaf': 1, 'estimator__max_depth': 2}\n",
    "üìä Confusion Matrix:\n",
    " [[19974     0]\n",
    " [   17     9]]\n",
    "\n",
    "üìã Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0      0.999     1.000     1.000     19974\n",
    "           1      1.000     0.346     0.514        26\n",
    "\n",
    "    accuracy                          0.999     20000\n",
    "   macro avg      1.000     0.673     0.757     20000\n",
    "weighted avg      0.999     0.999     0.999     20000\n",
    "\n",
    "---\n",
    "\n",
    "üî• Training and tuning: Logistic Regression\n",
    "‚úÖ Best Parameters for Logistic Regression:\n",
    " {'solver': 'saga', 'penalty': 'l1', 'max_iter': 1000, 'C': np.float64(10.0)}\n",
    "üìä Confusion Matrix:\n",
    " [[19974     0]\n",
    " [   14    12]]\n",
    "\n",
    "üìã Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0      0.999     1.000     1.000     19974\n",
    "           1      1.000     0.462     0.632        26\n",
    "\n",
    "    accuracy                          0.999     20000\n",
    "   macro avg      1.000     0.731     0.816     20000\n",
    "weighted avg      0.999     0.999     0.999     20000\n",
    "\n",
    "---\n",
    "\n",
    "üî• Training and tuning: K-Nearest Neighbors\n",
    "‚úÖ Best Parameters for K-Nearest Neighbors:\n",
    " {'weights': 'distance', 'p': 2, 'n_neighbors': 7, 'leaf_size': 30, 'algorithm': 'auto'}\n",
    "üìä Confusion Matrix:\n",
    " [[19973     1]\n",
    " [   12    14]]\n",
    "\n",
    "üìã Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0      0.999     1.000     1.000     19974\n",
    "           1      0.933     0.538     0.683        26\n",
    "\n",
    "    accuracy                          0.999     20000\n",
    "   macro avg      0.966     0.769     0.841     20000\n",
    "weighted avg      0.999     0.999     0.999     20000\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### Output of Evaluation Metrics and Best Parameters of Log Normalize and Dummy Encoded Transactions using Standard Scalar, and Wtihout Using SMOTE\n",
    "\n",
    "\n",
    "üî• Training and tuning: Random Forest\n",
    "‚úÖ Best Parameters for Random Forest:\n",
    " {'n_estimators': 400, 'min_samples_split': 5, 'max_features': 'log2', 'max_depth': 35, 'criterion': 'gini'}\n",
    "üìä Confusion Matrix:\n",
    " [[19973     1]\n",
    " [    8    18]]\n",
    "\n",
    "üìã Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0      1.000     1.000     1.000     19974\n",
    "           1      0.947     0.692     0.800        26\n",
    "\n",
    "    accuracy                          1.000     20000\n",
    "   macro avg      0.973     0.846     0.900     20000\n",
    "weighted avg      1.000     1.000     1.000     20000\n",
    "\n",
    "\n",
    "üî• Training and tuning: AdaBoost\n",
    "‚úÖ Best Parameters for AdaBoost:\n",
    " {'n_estimators': 50, 'learning_rate': 0.1, 'estimator__min_samples_split': 2, 'estimator__min_samples_leaf': 1, 'estimator__max_depth': 3}\n",
    "üìä Confusion Matrix:\n",
    " [[19973     1]\n",
    " [    9    17]]\n",
    "\n",
    "üìã Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0      1.000     1.000     1.000     19974\n",
    "           1      0.944     0.654     0.773        26\n",
    "\n",
    "    accuracy                          1.000     20000\n",
    "   macro avg      0.972     0.827     0.886     20000\n",
    "weighted avg      0.999     1.000     0.999     20000\n",
    "\n",
    "\n",
    "üî• Training and tuning: Logistic Regression\n",
    "‚úÖ Best Parameters for Logistic Regression:\n",
    " {'solver': 'saga', 'penalty': 'l1', 'max_iter': 1000, 'C': np.float64(10.0)}\n",
    "üìä Confusion Matrix:\n",
    " [[19973     1]\n",
    " [   12    14]]\n",
    "\n",
    "üìã Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0      0.999     1.000     1.000     19974\n",
    "           1      0.933     0.538     0.683        26\n",
    "\n",
    "    accuracy                          0.999     20000\n",
    "   macro avg      0.966     0.769     0.841     20000\n",
    "weighted avg      0.999     0.999     0.999     20000\n",
    "\n",
    "\n",
    "üî• Training and tuning: K-Nearest Neighbors\n",
    "‚úÖ Best Parameters for K-Nearest Neighbors:\n",
    " {'weights': 'distance', 'p': 1, 'n_neighbors': 7, 'leaf_size': 30, 'algorithm': 'auto'}\n",
    "üìä Confusion Matrix:\n",
    " [[19972     2]\n",
    " [    9    17]]\n",
    "\n",
    "üìã Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0      1.000     1.000     1.000     19974\n",
    "           1      0.895     0.654     0.756        26\n",
    "\n",
    "    accuracy                          0.999     20000\n",
    "   macro avg      0.947     0.827     0.878     20000\n",
    "weighted avg      0.999     0.999     0.999     20000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0c646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3.1: Separate features (X) and target (y)\n",
    "X = df_log_transacts.drop(columns=['isFraud'])  \n",
    "y = df_log_transacts['isFraud']                 \n",
    "\n",
    "# Step 3.2: Split into training and testing sets\n",
    "# Use stratify=y to keep the fraud/non-fraud ratio consistent, but I think could be reduntdant when using SMOTE??\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e4635c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud</th>\n",
       "      <th>amount_log</th>\n",
       "      <th>oldbalanceOrig_log</th>\n",
       "      <th>newbalanceOrig_log</th>\n",
       "      <th>oldbalanceDest_log</th>\n",
       "      <th>newbalanceDest_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.608846</td>\n",
       "      <td>11.522173</td>\n",
       "      <td>11.859486</td>\n",
       "      <td>12.400281</td>\n",
       "      <td>12.515597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>12.766230</td>\n",
       "      <td>8.528529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.457159</td>\n",
       "      <td>12.779584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7.707894</td>\n",
       "      <td>7.855409</td>\n",
       "      <td>5.871554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>12.043679</td>\n",
       "      <td>14.555915</td>\n",
       "      <td>14.633881</td>\n",
       "      <td>14.796901</td>\n",
       "      <td>14.866930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>12.473167</td>\n",
       "      <td>14.611048</td>\n",
       "      <td>14.722504</td>\n",
       "      <td>13.109396</td>\n",
       "      <td>12.355899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>1</td>\n",
       "      <td>12.774484</td>\n",
       "      <td>12.774484</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>1</td>\n",
       "      <td>12.799063</td>\n",
       "      <td>12.799063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.848402</td>\n",
       "      <td>13.517183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>1</td>\n",
       "      <td>12.862446</td>\n",
       "      <td>12.862446</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>1</td>\n",
       "      <td>13.689961</td>\n",
       "      <td>13.689961</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.012266</td>\n",
       "      <td>13.699217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>1</td>\n",
       "      <td>10.102874</td>\n",
       "      <td>10.102874</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.262824</td>\n",
       "      <td>14.278311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       isFraud  amount_log  oldbalanceOrig_log  newbalanceOrig_log  \\\n",
       "0            0   10.608846           11.522173           11.859486   \n",
       "1            0   12.766230            8.528529            0.000000   \n",
       "2            0    7.707894            7.855409            5.871554   \n",
       "3            0   12.043679           14.555915           14.633881   \n",
       "4            0   12.473167           14.611048           14.722504   \n",
       "...        ...         ...                 ...                 ...   \n",
       "99995        1   12.774484           12.774484            0.000000   \n",
       "99996        1   12.799063           12.799063            0.000000   \n",
       "99997        1   12.862446           12.862446            0.000000   \n",
       "99998        1   13.689961           13.689961            0.000000   \n",
       "99999        1   10.102874           10.102874            0.000000   \n",
       "\n",
       "       oldbalanceDest_log  newbalanceDest_log  \n",
       "0               12.400281           12.515597  \n",
       "1                8.457159           12.779584  \n",
       "2                0.000000            0.000000  \n",
       "3               14.796901           14.866930  \n",
       "4               13.109396           12.355899  \n",
       "...                   ...                 ...  \n",
       "99995            0.000000            0.000000  \n",
       "99996           12.848402           13.517183  \n",
       "99997            0.000000            0.000000  \n",
       "99998            9.012266           13.699217  \n",
       "99999           14.262824           14.278311  \n",
       "\n",
       "[100000 rows x 6 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log_transacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "417aea71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data pre-processing complete: SMOTE applied and features scaled.\n",
      "üìä Class balance after SMOTE:\n",
      " isFraud\n",
      "0    79887\n",
      "1    79887\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Balance the training set using SMOTE\n",
    "# SMOTE generates synthetic minority class (fraud) samples\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 4.1: Scale the features (mean=0, variance=1)\n",
    "# Fit scaler only on training data to prevent data leakage\n",
    "scaler = StandardScaler()\n",
    "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)  # Same scaler to test set\n",
    "\n",
    "print(\"‚úÖ Data pre-processing complete: SMOTE applied and features scaled.\")\n",
    "print(\"üìä Class balance after SMOTE:\\n\", y_resampled.value_counts())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c97f67",
   "metadata": {},
   "source": [
    "### 2) Search for best hyperparameters\n",
    "Use tools like GridSearchCV, RandomizedSearchCV, or model-specific tuning functions to find the best hyperparameters for your first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc48eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: hyperparameter Search Grids\n",
    "# With SMOTE \n",
    "param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [100, 200, 400],\n",
    "        \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"max_depth\": range(10, 55, 5),\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"min_samples_split\": [2, 5, 20]\n",
    "    },\n",
    "    \"AdaBoost\": {\n",
    "        \"n_estimators\": [50, 100, 200],\n",
    "        \"learning_rate\": [0.01, 0.1, 0.5, 1.0],\n",
    "        \"estimator__max_depth\": [1, 2, 3],              # ‚úÖ Controls weak learner complexity\n",
    "        \"estimator__min_samples_split\": [2, 5, 10],     # ‚úÖ Regularization\n",
    "        \"estimator__min_samples_leaf\": [1, 2, 4]\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"C\": np.logspace(-3, 3, 7),\n",
    "        \"penalty\": [\"l1\", \"l2\"],\n",
    "        \"solver\": [\"saga\"],                      \n",
    "        \"max_iter\": [500, 1000, 1500],                       \n",
    "    },\n",
    "    \"K-Nearest Neighbors\": {\n",
    "        \"n_neighbors\": [3, 5, 7, 11],\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"p\": [1, 2],\n",
    "        \"leaf_size\": [20, 30, 50],\n",
    "        \"algorithm\": [\"auto\"]\n",
    "    }\n",
    "} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30eee3",
   "metadata": {},
   "source": [
    "### 3) Train your model\n",
    "Select the model with best hyperparameters and generate predictions on your test set. Evaluate your models accuracy, precision, recall, and sensitivity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ed831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Training and tuning: Random Forest\n",
      "‚úÖ Best Parameters for Random Forest:\n",
      " {'n_estimators': 400, 'min_samples_split': 5, 'max_features': 'log2', 'max_depth': 35, 'criterion': 'gini'}\n",
      "üìä Confusion Matrix:\n",
      " [[19899    84]\n",
      " [    2    15]]\n",
      "\n",
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.996     0.998     19983\n",
      "           1      0.152     0.882     0.259        17\n",
      "\n",
      "    accuracy                          0.996     20000\n",
      "   macro avg      0.576     0.939     0.628     20000\n",
      "weighted avg      0.999     0.996     0.997     20000\n",
      "\n",
      "\n",
      "üî• Training and tuning: AdaBoost\n",
      "‚úÖ Best Parameters for AdaBoost:\n",
      " {'n_estimators': 50, 'learning_rate': 0.5, 'estimator__min_samples_split': 5, 'estimator__min_samples_leaf': 1, 'estimator__max_depth': 2}\n",
      "üìä Confusion Matrix:\n",
      " [[19137   846]\n",
      " [    3    14]]\n",
      "\n",
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.958     0.978     19983\n",
      "           1      0.016     0.824     0.032        17\n",
      "\n",
      "    accuracy                          0.958     20000\n",
      "   macro avg      0.508     0.891     0.505     20000\n",
      "weighted avg      0.999     0.958     0.977     20000\n",
      "\n",
      "\n",
      "üî• Training and tuning: Logistic Regression\n",
      "‚úÖ Best Parameters for Logistic Regression:\n",
      " {'solver': 'saga', 'penalty': 'l1', 'max_iter': 1000, 'C': np.float64(10.0)}\n",
      "üìä Confusion Matrix:\n",
      " [[18344  1639]\n",
      " [    3    14]]\n",
      "\n",
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.918     0.957     19983\n",
      "           1      0.008     0.824     0.017        17\n",
      "\n",
      "    accuracy                          0.918     20000\n",
      "   macro avg      0.504     0.871     0.487     20000\n",
      "weighted avg      0.999     0.918     0.956     20000\n",
      "\n",
      "\n",
      "üî• Training and tuning: K-Nearest Neighbors\n",
      "‚úÖ Best Parameters for K-Nearest Neighbors:\n",
      " {'weights': 'distance', 'p': 2, 'n_neighbors': 3, 'leaf_size': 30, 'algorithm': 'auto'}\n",
      "üìä Confusion Matrix:\n",
      " [[19858   125]\n",
      " [    1    16]]\n",
      "\n",
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.994     0.997     19983\n",
      "           1      0.113     0.941     0.203        17\n",
      "\n",
      "    accuracy                          0.994     20000\n",
      "   macro avg      0.557     0.967     0.600     20000\n",
      "weighted avg      0.999     0.994     0.996     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Define classifiers for training\n",
    "# With SMOTE\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(estimator=DecisionTreeClassifier(random_state=42), random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(solver='saga', random_state=42),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier()  \n",
    "}\n",
    "\n",
    "# Store best models \n",
    "best_models = {}\n",
    "\n",
    "# Select models for training and tuning\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüî• Training and tuning: {name}\")\n",
    "    \n",
    "    param_grid = param_grids[name] # Select which param to use in randomized search\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_grid,\n",
    "        #n_iter=10,      # Tuning param\n",
    "        cv=5,\n",
    "        scoring='f1',    # Scoring param, maybe I can also try 'roc_auc'??\n",
    "        random_state=42,\n",
    "        n_jobs=-1        # Number of CPU-Cores param \n",
    "    )\n",
    "    \n",
    "    # Fit to training data\n",
    "    random_search.fit(X_resampled_scaled, y_resampled)\n",
    "    \n",
    "    # Save the best model\n",
    "    best_models[name] = random_search.best_estimator_\n",
    "\n",
    "    # Predict on test set using the best model\n",
    "    yhat = best_models[name].predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    cm = confusion_matrix(y_test, yhat)\n",
    "    report = classification_report(y_test, yhat, digits=3, zero_division=0)\n",
    "    \n",
    "    print(f\"‚úÖ Best Parameters for {name}:\\n\", random_search.best_params_)\n",
    "    print(\"üìä Confusion Matrix:\\n\", cm)\n",
    "    print(\"\\nüìã Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bac6a7",
   "metadata": {},
   "source": [
    "## training and evaluation metrics using baseline.csv (no transformations)\n",
    "üí• Training and evaluating: Random Forest\n",
    "üìä Confusion Matrix:\n",
    " [[39820   128]  ‚Üí True Negatives / False Positives\n",
    " [   11    41]]  ‚Üí False Negatives / True Positives\n",
    "\n",
    "\n",
    "üìÑ Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0      1.000     0.997     0.998     39948\n",
    "           1      0.243     0.788     0.371        52\n",
    "\n",
    "    accuracy                          0.997     40000\n",
    "   macro avg      0.621     0.893     0.685     40000\n",
    "weighted avg      0.999     0.997     0.997     40000\n",
    "\n",
    "\n",
    "üí• Training and evaluating: AdaBoost\n",
    "üìä Confusion Matrix:\n",
    " [[37111  2837] ‚Üí True Negatives / False Positives\n",
    " [    1    51]] ‚Üí False Negatives / True Positives\n",
    "\n",
    "\n",
    "üìÑ Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0      1.000     0.929     0.963     39948\n",
    "           1      0.018     0.981     0.035        52\n",
    "\n",
    "    accuracy                          0.929     40000\n",
    "   macro avg      0.509     0.955     0.499     40000\n",
    "weighted avg      0.999     0.929     0.962     40000\n",
    "\n",
    "\n",
    "üí• Training and evaluating: Logistic Regression\n",
    "üìä Confusion Matrix:\n",
    " [[39049   899]\n",
    " [    5    47]]\n",
    "\n",
    "üìÑ Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0      1.000     0.977     0.989     39948\n",
    "           1      0.050     0.904     0.094        52\n",
    "\n",
    "    accuracy                          0.977     40000\n",
    "   macro avg      0.525     0.941     0.541     40000\n",
    "weighted avg      0.999     0.977     0.987     40000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db128d64",
   "metadata": {},
   "source": [
    "## Second Model\n",
    "\n",
    "Create a second machine learning object and rerun steps (2) & (3) on this model. Compare accuracy metrics between these two models. Which handles the class imbalance more effectively?\n",
    "\n",
    "Create as many code-blocks as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732baab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are three models in a dictionary in the above cell..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66c411",
   "metadata": {},
   "source": [
    "### (Bonus/Optional) Third Model\n",
    "\n",
    "Create a third machine learning model and rerun steps (2) & (3) on this model. Which model has the best predictive capabilities? \n",
    "\n",
    "Create as many code-blocks as needed.\n",
    "\n",
    "\n",
    "Based on first run: the best model so far is, ***Support Vector Machine** with the following configuration and results:\n",
    "\n",
    "- **Best Parameters**:\n",
    "  ```python\n",
    "  {\n",
    "      C': np.logspace(-1, 2, 5),            \n",
    "    'kernel': ['rbf', 'sigmoid'],          \n",
    "    'gamma': ['scale', 'auto'] \n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **Confusion Matrix**:\n",
    "  ```\n",
    "  [[39867    81] ‚Üí True Negatives / False Positives\n",
    "  [    30    22]] ‚Üí False Negatives / True Positives\n",
    "  ```\n",
    "\n",
    "- **Classification Report**:\n",
    "\n",
    "  | Class | Precision | Recall | F1-Score | Support |\n",
    "  |-------|-----------|--------|----------|---------|\n",
    "  | 0     | 0         | 1.00   | 1.00     | 39948   |\n",
    "  | 1     | 1         | 0.42   | 0.59     | 52      |\n",
    "\n",
    "- This shows that Support Vector Machine performs **pretty well** at identifying non-fraud, and achieves strong *F1-Score** for fraud despite class imbalance. I'll continue to tune model parameters, use various data transform datasets, etc., in order to achieve even better precision-recall trade-off.\n",
    "- The dataset used: **baseline_dummy_encode_transacts**\n",
    "   - consisting of the **baseline--non-scaled--columns** and dummy encodings of type column. \n",
    "\n",
    "---\n",
    "\n",
    "Based on second run: the best model so far is, **Support Vector Machine** with the following configuration and results:\n",
    "\n",
    "- **Best Parameters**:\n",
    "  ```python\n",
    "  {\n",
    "      C': np.logspace(-2, 2, 5),            \n",
    "    'kernel': ['rbf', 'sigmoid'],          \n",
    "    'gamma': ['scale', 'auto'] \n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **Confusion Matrix**:\n",
    "  ```\n",
    "  [[29960     1] ‚Üí True Negatives / False Positives\n",
    "  [    16    23]] ‚Üí False Negatives / True Positives\n",
    "  ```\n",
    "\n",
    "- **Classification Report**:\n",
    "\n",
    "  | Class | Precision | Recall | F1-Score | Support |\n",
    "  |-------|-----------|--------|----------|---------|\n",
    "  | 0     | 1.00      | 1.00   | 1.00     | 29961   |\n",
    "  | 1     | 0.96      | 0.96   | 0.75     | 39      |\n",
    "\n",
    "- This shows that Support Vector Machine performs **pretty well** at identifying non-fraud, and achieves strong *F1-Score** for fraud despite class imbalance. I'll continue to tune model parameters, use various data transform datasets, etc., in order to achieve even better precision-recall trade-off.\n",
    "- The dataset used: log_transacts\n",
    "   - consisting of **log-transformed** columns only. \n",
    "**Note:** Support is at 39--maybe due to the following warning: ConvergenceWarning: Solver terminated early (max_iter=10000).  Consider pre-processing your data with StandardScaler or MinMaxScaler. I'll try increasing max_iter=20000...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7922cd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Separate features (X) and target (y)\n",
    "X = df_log_transacts.drop(columns=['isFraud'])  # All features except target\n",
    "y = df_log_transacts['isFraud']                 # Target variable\n",
    "\n",
    "# Step 3: Split into training and testing sets\n",
    "# Use stratify=y to keep the fraud/non-fraud ratio consistent, but I think could be reduntdant when using SMOTE??\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0460f779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data pre-processing complete: SMOTE applied and features scaled.\n",
      "üìä Class balance after SMOTE:\n",
      " isFraud\n",
      "0    79896\n",
      "1    79896\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Scale features (mean=0, variance=1)\n",
    "# Fit scaler only on training data to prevent data leakage/peeking\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_resampled_scaled = scaler.fit_transform(X_resampled)  # Fit only on training data\n",
    "X_test_scaled = scaler.transform(X_test)                # Transform test data\n",
    "\n",
    "print(\"‚úÖ Data pre-processing complete: SMOTE applied and features scaled.\")\n",
    "print(\"üìä Class balance after SMOTE:\\n\", y_resampled.value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4e8f281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oneps\\anaconda3\\envs\\ds\\Lib\\site-packages\\sklearn\\svm\\_base.py:305: ConvergenceWarning: Solver terminated early (max_iter=15000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\oneps\\\\anaconda3\\\\envs\\\\ds\\\\Lib\\\\site-packages\\\\sklearn\\\\utils\\\\_repr_html\\\\estimator.js'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\oneps\\anaconda3\\envs\\ds\\Lib\\site-packages\\IPython\\core\\formatters.py:1036\u001b[0m, in \u001b[0;36mMimeBundleFormatter.__call__\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m   1033\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[0;32m   1035\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1036\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m method(include\u001b[38;5;241m=\u001b[39minclude, exclude\u001b[38;5;241m=\u001b[39mexclude)\n\u001b[0;32m   1037\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\oneps\\anaconda3\\envs\\ds\\Lib\\site-packages\\sklearn\\utils\\_repr_html\\base.py:151\u001b[0m, in \u001b[0;36m_repr_mimebundle_\u001b[1;34m(self, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\oneps\\anaconda3\\envs\\ds\\Lib\\site-packages\\sklearn\\utils\\_repr_html\\estimator.py:489\u001b[0m, in \u001b[0;36mestimator_html_repr\u001b[1;34m(estimator)\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\oneps\\\\anaconda3\\\\envs\\\\ds\\\\Lib\\\\site-packages\\\\sklearn\\\\utils\\\\_repr_html\\\\estimator.js'"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\oneps\\\\anaconda3\\\\envs\\\\ds\\\\Lib\\\\site-packages\\\\sklearn\\\\utils\\\\_repr_html\\\\estimator.js'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\oneps\\anaconda3\\envs\\ds\\Lib\\site-packages\\IPython\\core\\formatters.py:406\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    404\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m method()\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\oneps\\anaconda3\\envs\\ds\\Lib\\site-packages\\sklearn\\utils\\_repr_html\\base.py:145\u001b[0m, in \u001b[0;36m_repr_html_inner\u001b[1;34m(self)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\oneps\\anaconda3\\envs\\ds\\Lib\\site-packages\\sklearn\\utils\\_repr_html\\estimator.py:489\u001b[0m, in \u001b[0;36mestimator_html_repr\u001b[1;34m(estimator)\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\oneps\\\\anaconda3\\\\envs\\\\ds\\\\Lib\\\\site-packages\\\\sklearn\\\\utils\\\\_repr_html\\\\estimator.js'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=SVC(max_iter=15000, random_state=42),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'C': array([  0.1       ,   0.56234133,   3.16227766,  17.7827941 ,\n",
       "       100.        ]),\n",
       "                                        'gamma': ['scale', 'auto']},\n",
       "                   random_state=42, scoring='f1')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement random search on the LinearSVC model to find best hyperparams\n",
    "svc_params_grid = {\n",
    "    'C': np.logspace(-1, 2, 5), # (0.1, 10, 20)\n",
    "    # kernel': ['rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "#  \n",
    "svc = SVC(max_iter=15000, tol=1e-3, random_state=42)\n",
    "\n",
    "# set up RandomizedSearchCV with params like 5-fold, cross-validation, etc.\n",
    "random_search = RandomizedSearchCV(\n",
    "    svc, # removed estimator=svc\n",
    "    param_distributions=svc_params_grid,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# fit this model on scaled training data\n",
    "random_search.fit(X_resampled_scaled, y_resampled)\n",
    "\n",
    "#yhat = random_search.best_estimator_.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8abd5718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Confusion Matrix:\n",
      " [[19746   237]\n",
      " [    0    17]]\n",
      "\n",
      "‚úÖ Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     19983\n",
      "           1       0.07      1.00      0.13        17\n",
      "\n",
      "    accuracy                           0.99     20000\n",
      "   macro avg       0.53      0.99      0.56     20000\n",
      "weighted avg       1.00      0.99      0.99     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# best model after tuning\n",
    "best_svc = random_search.best_estimator_\n",
    "\n",
    "# scaled test data for predictions\n",
    "yhat = best_svc.predict(X_test_scaled)\n",
    "\n",
    "confusion = confusion_matrix(y_test, yhat)\n",
    "class_report = classification_report(y_test, yhat)\n",
    "\n",
    "print(\"‚úÖ Confusion Matrix:\\n\", confusion)\n",
    "print(\"\\n‚úÖ Classification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9bc18926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data pre-processing complete: SMOTE applied and features scaled.\n",
      "üìä Class balance after SMOTE:\n",
      " isFraud\n",
      "0    79896\n",
      "1    79896\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SMOTE generates synthetic minority class (fraud) samples\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 5: Scale features (mean=0, variance=1)\n",
    "# Fit scaler only on training data to prevent data leakage/peeking\n",
    "scaler = StandardScaler()\n",
    "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)  # Apply same scaler to test set\n",
    "\n",
    "print(\"‚úÖ Data pre-processing complete: SMOTE applied and features scaled.\")\n",
    "print(\"üìä Class balance after SMOTE:\\n\", y_resampled.value_counts())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6fa586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix \n",
      " [[19974     0]\n",
      " [   19     7]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19974\n",
      "           1       1.00      0.27      0.42        26\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       1.00      0.63      0.71     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize a Support Vector Classifier with RBF kernel to handle non-linearity\n",
    "svc_non_linear = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "\n",
    "# Train the classifier on the XOR dataset\n",
    "svc_non_linear.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the same dataset\n",
    "yhat = svc_non_linear.predict(X_test)\n",
    "\n",
    "confusion = confusion_matrix(y_test, yhat)\n",
    "class_report = classification_report(y_test, yhat)\n",
    "\n",
    "print(\"Confusion Matrix \\n\", confusion)\n",
    "print(\"\\nClassification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement random search on the LinearSVC model to find best hyperparams\n",
    "svc_params_grid = {\n",
    "    'C': np.logspace(-2, 2, 5),            \n",
    "    'kernel': ['rbf', 'sigmoid'],          \n",
    "    'gamma': ['scale', 'auto'],\n",
    "    #'degree': [2, 3, 4, 5] \n",
    "}\n",
    "\n",
    "svc = SVC(max_iter=15000, random_state=42)\n",
    "\n",
    "# set up RandomizedSearchCV with 5-fold cross-validation\n",
    "random_search = RandomizedSearchCV(svc, param_distributions=svc_params_grid, scoring='f1', n_jobs=-1, cv=5, random_state=42)\n",
    "\n",
    "# fit this model on your training data\n",
    "random_search.fit(X_train, y_train)  # ran for 36m 12.0s without n_jobs param being set and took 16m 13.6s when scaling before training and using all 8 cores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1a36313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix \n",
      " [[19981     2]\n",
      " [    7    10]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     19983\n",
      "           1       0.83      0.59      0.69        17\n",
      "\n",
      "    accuracy                           1.00     20000\n",
      "   macro avg       0.92      0.79      0.84     20000\n",
      "weighted avg       1.00      1.00      1.00     20000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_svc = random_search.best_estimator_\n",
    "\n",
    "# make predictions on the same dataset\n",
    "yhat = best_svc.predict(X_test)\n",
    "\n",
    "confusion = confusion_matrix(y_test, yhat)\n",
    "class_report = classification_report(y_test, yhat)\n",
    "\n",
    "print(\"Confusion Matrix \\n\", confusion)\n",
    "print(\"\\nClassification Report:\\n\", class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
