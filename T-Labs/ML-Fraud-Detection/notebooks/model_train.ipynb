{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5bd66ee",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "In this notebook, we will ask you a series of questions regarding model selection. Based on your responses, we will ask you to create the ML models that you've chosen. \n",
    "\n",
    "The bonus step is completely optional, but if you provide a sufficient third machine learning model in this project, we will add `1000` points to your Kahoot leaderboard score.\n",
    "\n",
    "**Note**: Use the dataset that you've created in your previous data transformation step (not the original model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641dd43",
   "metadata": {},
   "source": [
    "## AI/ML Model Training, Testing, and Evaluation Steps/workflow:\n",
    "1. Import the necessary libraries\n",
    "2. Load data\n",
    "3. Perform EDA\n",
    "4. Preprocess data\n",
    "5. Train the model\n",
    "   - train/test/split\n",
    "   - Search\n",
    "   - Make prediction , and then \n",
    "6. Evaluate the model\n",
    "\n",
    "**More Granular:**\n",
    "- ‚òëÔ∏è Import necessary Python libraries and or modules\n",
    "- ‚òëÔ∏è Load data\n",
    "- ‚òëÔ∏è Split train/test (keep fraud ratio with stratify)\n",
    "- ‚òëÔ∏è Balance minority fraud class with SMOTE\n",
    "- ‚òëÔ∏è Scale features for all models\n",
    "- ‚òëÔ∏è Define classifiers in a dictionary \n",
    "    -  Not neccessary for model trainging, but worth doing for a more streamlined approach.\n",
    "- ‚òëÔ∏è Loop: search for best parameters using RandomizedSearchCV\n",
    "- ‚òëÔ∏è Loop: train, predict, evaluate\n",
    "- ‚òëÔ∏è Print Confusion Matrix & Classification Report for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9894067c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úîÔ∏èThere are currently 8 CPU-Cores available for model trainingüí™\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries and modules\n",
    "# Panda for handliing dataframes and numpy for handling arrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Preprocessing SMOTE Oversampling and Scaling module \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Max num n_jobs/processor(s) that's available for used\n",
    "from joblib import cpu_count\n",
    "print(f\"‚úîÔ∏èThere are currently {cpu_count()} CPU-Cores available for model trainingüí™\")\n",
    "\n",
    "# Load evaluation metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    " \n",
    "# Import modules for data manipulation\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a9a55e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csv files to pandas dataframe\n",
    "# Baseline transactions \n",
    "df_baseline = pd.read_csv(\"../data/baseline_transacts.csv\")\n",
    "\n",
    "# Baseline dummy encoding transform\n",
    "df_base_dummy_encode = pd.read_csv(\"../data/baseline_dummy_encode_transacts.csv\")\n",
    "\n",
    "# Log feature transform\n",
    "df_log_transacts = pd.read_csv(\"../data/log_transacts.csv\")\n",
    "\n",
    "# Log and dummy encoding transforms\n",
    "df_log_dummy_encode_transacts = pd.read_csv(\"../data/log_dummy_encode_transacts.csv\")\n",
    "\n",
    "\n",
    "# Log, change in transaction, and dummy encoding transforms\n",
    "df_log_diff_dummy_encode = pd.read_csv(\"../data/log_diff_dummy_transacts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c63bcfea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isFraud</th>\n",
       "      <th>amount_log</th>\n",
       "      <th>oldbalanceOrig_log</th>\n",
       "      <th>newbalanceOrig_log</th>\n",
       "      <th>oldbalanceDest_log</th>\n",
       "      <th>newbalanceDest_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10.608846</td>\n",
       "      <td>11.522173</td>\n",
       "      <td>11.859486</td>\n",
       "      <td>12.400281</td>\n",
       "      <td>12.515597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>12.766230</td>\n",
       "      <td>8.528529</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.457159</td>\n",
       "      <td>12.779584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>7.707894</td>\n",
       "      <td>7.855409</td>\n",
       "      <td>5.871554</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>12.043679</td>\n",
       "      <td>14.555915</td>\n",
       "      <td>14.633881</td>\n",
       "      <td>14.796901</td>\n",
       "      <td>14.866930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>12.473167</td>\n",
       "      <td>14.611048</td>\n",
       "      <td>14.722504</td>\n",
       "      <td>13.109396</td>\n",
       "      <td>12.355899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149995</th>\n",
       "      <td>1</td>\n",
       "      <td>14.723745</td>\n",
       "      <td>14.723745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149996</th>\n",
       "      <td>1</td>\n",
       "      <td>13.473565</td>\n",
       "      <td>13.473565</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.406430</td>\n",
       "      <td>14.133707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149997</th>\n",
       "      <td>1</td>\n",
       "      <td>10.117585</td>\n",
       "      <td>10.117585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149998</th>\n",
       "      <td>1</td>\n",
       "      <td>13.573852</td>\n",
       "      <td>13.573852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.597156</td>\n",
       "      <td>14.358291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149999</th>\n",
       "      <td>1</td>\n",
       "      <td>14.070303</td>\n",
       "      <td>14.070303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150000 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        isFraud  amount_log  oldbalanceOrig_log  newbalanceOrig_log  \\\n",
       "0             0   10.608846           11.522173           11.859486   \n",
       "1             0   12.766230            8.528529            0.000000   \n",
       "2             0    7.707894            7.855409            5.871554   \n",
       "3             0   12.043679           14.555915           14.633881   \n",
       "4             0   12.473167           14.611048           14.722504   \n",
       "...         ...         ...                 ...                 ...   \n",
       "149995        1   14.723745           14.723745            0.000000   \n",
       "149996        1   13.473565           13.473565            0.000000   \n",
       "149997        1   10.117585           10.117585            0.000000   \n",
       "149998        1   13.573852           13.573852            0.000000   \n",
       "149999        1   14.070303           14.070303            0.000000   \n",
       "\n",
       "        oldbalanceDest_log  newbalanceDest_log  \n",
       "0                12.400281           12.515597  \n",
       "1                 8.457159           12.779584  \n",
       "2                 0.000000            0.000000  \n",
       "3                14.796901           14.866930  \n",
       "4                13.109396           12.355899  \n",
       "...                    ...                 ...  \n",
       "149995            0.000000            0.000000  \n",
       "149996           13.406430           14.133707  \n",
       "149997            0.000000            0.000000  \n",
       "149998           13.597156           14.358291  \n",
       "149999            0.000000            0.000000  \n",
       "\n",
       "[150000 rows x 6 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first five rows\n",
    "df_log_transacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe47df31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount_log</th>\n",
       "      <th>oldbalanceOrig_log</th>\n",
       "      <th>newbalanceOrig_log</th>\n",
       "      <th>oldbalanceDest_log</th>\n",
       "      <th>newbalanceDest_log</th>\n",
       "      <th>isFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199741</th>\n",
       "      <td>14.928063</td>\n",
       "      <td>14.928063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199742</th>\n",
       "      <td>16.118096</td>\n",
       "      <td>16.118096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.118096</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199743</th>\n",
       "      <td>8.352894</td>\n",
       "      <td>8.352894</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199744</th>\n",
       "      <td>9.080735</td>\n",
       "      <td>9.080735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.080735</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199745</th>\n",
       "      <td>12.582867</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.582867</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199995</th>\n",
       "      <td>11.546066</td>\n",
       "      <td>11.546066</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.546066</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>14.366892</td>\n",
       "      <td>14.366892</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>16.118096</td>\n",
       "      <td>16.118096</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.259846</td>\n",
       "      <td>16.138981</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>13.974011</td>\n",
       "      <td>13.974011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>14.611532</td>\n",
       "      <td>14.611532</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>259 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        amount_log  oldbalanceOrig_log  newbalanceOrig_log  \\\n",
       "199741   14.928063           14.928063                 0.0   \n",
       "199742   16.118096           16.118096                 0.0   \n",
       "199743    8.352894            8.352894                 0.0   \n",
       "199744    9.080735            9.080735                 0.0   \n",
       "199745   12.582867            0.000000                 0.0   \n",
       "...            ...                 ...                 ...   \n",
       "199995   11.546066           11.546066                 0.0   \n",
       "199996   14.366892           14.366892                 0.0   \n",
       "199997   16.118096           16.118096                 0.0   \n",
       "199998   13.974011           13.974011                 0.0   \n",
       "199999   14.611532           14.611532                 0.0   \n",
       "\n",
       "        oldbalanceDest_log  newbalanceDest_log  isFraud  \n",
       "199741            0.000000            0.000000        1  \n",
       "199742            0.000000           16.118096        1  \n",
       "199743            0.000000            0.000000        1  \n",
       "199744            0.000000            9.080735        1  \n",
       "199745            0.000000           12.582867        1  \n",
       "...                    ...                 ...      ...  \n",
       "199995            0.000000           11.546066        1  \n",
       "199996            0.000000            0.000000        1  \n",
       "199997           12.259846           16.138981        1  \n",
       "199998            0.000000            0.000000        1  \n",
       "199999            0.000000            0.000000        1  \n",
       "\n",
       "[259 rows x 6 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log_transacts[df_log_transacts['isFraud'] == 1] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25b90a0",
   "metadata": {},
   "source": [
    "## Questions\n",
    "Is this a classification or regression task?  \n",
    "\n",
    "This is a classification task since we are effectively assigning one of two cases to either respective 'bin' of fraud and not-fraud (i.e., 0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017bfb9f",
   "metadata": {},
   "source": [
    "Are you predicting for multiple classes or binary classes?  \n",
    "\n",
    "Since this is a binary classification problem, where we are trying to find out if transactions are either fraud or not-fraud, there are only 2 possible classes/labelings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd9378",
   "metadata": {},
   "source": [
    "Given these observations, which 2 (or possibly 3) machine learning models will you choose?  \n",
    "\n",
    "I'm using Logistic Regression, Support Vector Machine (SVM), and AdaBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c408b67",
   "metadata": {},
   "source": [
    "## First Model\n",
    "\n",
    "Using the first model that you've chosen, implement the following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fab3d0",
   "metadata": {},
   "source": [
    "### 1) Create a train-test split\n",
    "\n",
    "Use your cleaned and transformed dataset to divide your features and labels into training and testing sets. Make sure you‚Äôre only using numeric or properly encoded features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b0c646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Separate features (X) and target (y)\n",
    "X = df_log_transacts.drop(columns=['isFraud'])  # All features except target\n",
    "y = df_log_transacts['isFraud']                 # Target variable\n",
    "\n",
    "# Step 3: Split into training and testing sets\n",
    "# Use stratify=y to keep the fraud/non-fraud ratio consistent, but I think could be reduntdant when using SMOTE??\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "417aea71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data pre-processing complete: SMOTE applied and features scaled.\n",
      "üìä Class balance after SMOTE:\n",
      " isFraud\n",
      "0    119844\n",
      "1    119844\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Balance the training set using SMOTE\n",
    "# SMOTE generates synthetic minority class (fraud) samples\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 5: Scale the features (mean=0, variance=1)\n",
    "# Fit scaler only on training data to prevent data leakage\n",
    "scaler = StandardScaler()\n",
    "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)  # Apply same scaler to test set\n",
    "\n",
    "print(\"‚úÖ Data pre-processing complete: SMOTE applied and features scaled.\")\n",
    "print(\"üìä Class balance after SMOTE:\\n\", y_resampled.value_counts())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c97f67",
   "metadata": {},
   "source": [
    "### 2) Search for best hyperparameters\n",
    "Use tools like GridSearchCV, RandomizedSearchCV, or model-specific tuning functions to find the best hyperparameters for your first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fc48eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: hyperparameter Search Grids\n",
    "param_grids = {\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [100, 200, 400, 600],\n",
    "        \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        \"max_depth\": range(10, 55, 5),\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"min_samples_split\": [2, 5, 20]\n",
    "    },\n",
    "    \"AdaBoost\": {\n",
    "        \"n_estimators\": [50, 100, 200, 400],\n",
    "        \"learning_rate\": [0.01, 0.1, 1, 2, 10, 100]\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"C\": np.logspace(-3, 3, 7),\n",
    "        \"penalty\": [\"l1\", \"l2\"],\n",
    "        \"solver\": [\"liblinear\"]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30eee3",
   "metadata": {},
   "source": [
    "### 3) Train your model\n",
    "Select the model with best hyperparameters and generate predictions on your test set. Evaluate your models accuracy, precision, recall, and sensitivity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "228ed831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• Training and tuning: Random Forest\n",
      "‚úÖ Best Parameters for Random Forest:\n",
      " {'n_estimators': 100, 'min_samples_split': 5, 'max_features': 'sqrt', 'max_depth': 30, 'criterion': 'gini'}\n",
      "üìä Confusion Matrix:\n",
      " [[39867    81]\n",
      " [    5    47]]\n",
      "\n",
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.998     0.999     39948\n",
      "           1      0.367     0.904     0.522        52\n",
      "\n",
      "    accuracy                          0.998     40000\n",
      "   macro avg      0.684     0.951     0.761     40000\n",
      "weighted avg      0.999     0.998     0.998     40000\n",
      "\n",
      "\n",
      "üî• Training and tuning: AdaBoost\n",
      "‚úÖ Best Parameters for AdaBoost:\n",
      " {'n_estimators': 200, 'learning_rate': 1}\n",
      "üìä Confusion Matrix:\n",
      " [[38875  1073]\n",
      " [    1    51]]\n",
      "\n",
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.973     0.986     39948\n",
      "           1      0.045     0.981     0.087        52\n",
      "\n",
      "    accuracy                          0.973     40000\n",
      "   macro avg      0.523     0.977     0.537     40000\n",
      "weighted avg      0.999     0.973     0.985     40000\n",
      "\n",
      "\n",
      "üî• Training and tuning: Logistic Regression\n",
      "‚úÖ Best Parameters for Logistic Regression:\n",
      " {'solver': 'liblinear', 'penalty': 'l2', 'C': np.float64(1000.0)}\n",
      "üìä Confusion Matrix:\n",
      " [[38663  1285]\n",
      " [    0    52]]\n",
      "\n",
      "üìã Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0      1.000     0.968     0.984     39948\n",
      "           1      0.039     1.000     0.075        52\n",
      "\n",
      "    accuracy                          0.968     40000\n",
      "   macro avg      0.519     0.984     0.529     40000\n",
      "weighted avg      0.999     0.968     0.982     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Step 6: Define classifiers\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"AdaBoost\": AdaBoostClassifier(random_state=42),\n",
    "    \"Logistic Regression\": LogisticRegression(solver='liblinear', random_state=42)\n",
    "}\n",
    "\n",
    "# Store best models for future use and less compute\n",
    "best_models = {}\n",
    "\n",
    "# Select models for training and tuning\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nüî• Training and tuning: {name}\")\n",
    "    \n",
    "    param_grid = param_grids[name] # Select which param to use in randomized search\n",
    "    \n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=10,  # Tuning param\n",
    "        cv=5,\n",
    "        scoring='f1',  # Scoring param, maybe I can also try 'roc_auc'??\n",
    "        random_state=42,\n",
    "        n_jobs=-1 # Number of CPU-Cores param \n",
    "    )\n",
    "    \n",
    "    # Fit to training data\n",
    "    random_search.fit(X_resampled_scaled, y_resampled)\n",
    "    \n",
    "    # Save the best model\n",
    "    best_models[name] = random_search.best_estimator_\n",
    "\n",
    "    # Predict on test set using the best model\n",
    "    yhat = best_models[name].predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate\n",
    "    cm = confusion_matrix(y_test, yhat)\n",
    "    report = classification_report(y_test, yhat, digits=3)\n",
    "    \n",
    "    print(f\"‚úÖ Best Parameters for {name}:\\n\", random_search.best_params_)\n",
    "    print(\"üìä Confusion Matrix:\\n\", cm)\n",
    "    print(\"\\nüìã Classification Report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bac6a7",
   "metadata": {},
   "source": [
    "## training and evaluation metrics using baseline.csv (no transformations)\n",
    "üí• Training and evaluating: Random Forest\n",
    "üìä Confusion Matrix:\n",
    " [[39820   128]  ‚Üí True Negatives / False Positives\n",
    " [   11    41]]  ‚Üí False Negatives / True Positives\n",
    "\n",
    "\n",
    "üìÑ Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0      1.000     0.997     0.998     39948\n",
    "           1      0.243     0.788     0.371        52\n",
    "\n",
    "    accuracy                          0.997     40000\n",
    "   macro avg      0.621     0.893     0.685     40000\n",
    "weighted avg      0.999     0.997     0.997     40000\n",
    "\n",
    "\n",
    "üí• Training and evaluating: AdaBoost\n",
    "üìä Confusion Matrix:\n",
    " [[37111  2837] ‚Üí True Negatives / False Positives\n",
    " [    1    51]] ‚Üí False Negatives / True Positives\n",
    "\n",
    "\n",
    "üìÑ Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0      1.000     0.929     0.963     39948\n",
    "           1      0.018     0.981     0.035        52\n",
    "\n",
    "    accuracy                          0.929     40000\n",
    "   macro avg      0.509     0.955     0.499     40000\n",
    "weighted avg      0.999     0.929     0.962     40000\n",
    "\n",
    "\n",
    "üí• Training and evaluating: Logistic Regression\n",
    "üìä Confusion Matrix:\n",
    " [[39049   899]\n",
    " [    5    47]]\n",
    "\n",
    "üìÑ Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0      1.000     0.977     0.989     39948\n",
    "           1      0.050     0.904     0.094        52\n",
    "\n",
    "    accuracy                          0.977     40000\n",
    "   macro avg      0.525     0.941     0.541     40000\n",
    "weighted avg      0.999     0.977     0.987     40000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db128d64",
   "metadata": {},
   "source": [
    "## Second Model\n",
    "\n",
    "Create a second machine learning object and rerun steps (2) & (3) on this model. Compare accuracy metrics between these two models. Which handles the class imbalance more effectively?\n",
    "\n",
    "Create as many code-blocks as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732baab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are three models in a dictionary in the above cell..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66c411",
   "metadata": {},
   "source": [
    "### (Bonus/Optional) Third Model\n",
    "\n",
    "Create a third machine learning model and rerun steps (2) & (3) on this model. Which model has the best predictive capabilities? \n",
    "\n",
    "Create as many code-blocks as needed.\n",
    "\n",
    "\n",
    "The best model so far is ***Support Vector Machine** with the following configuration and results:\n",
    "\n",
    "- **Best Parameters**:\n",
    "  ```python\n",
    "  {\n",
    "      C': np.logspace(-1, 2, 5),            \n",
    "    'kernel': ['rbf', 'sigmoid'],          \n",
    "    'gamma': ['scale', 'auto'] \n",
    "  }\n",
    "  ```\n",
    "\n",
    "- **Confusion Matrix**:\n",
    "  ```\n",
    "  [[39867    81]\n",
    "  [    30    22]]\n",
    "  ```\n",
    "\n",
    "- **Classification Report**:\n",
    "\n",
    "  | Class | Precision | Recall | F1-Score | Support |\n",
    "  |-------|-----------|--------|----------|---------|\n",
    "  | 0     | 0         | 1.00   | 1.00     | 39948   |\n",
    "  | 1     | 1         | 0.42   | 0.59     | 52      |\n",
    "\n",
    "- This shows that Support Vector Machine performs **pretty well** at identifying non-fraud, and achieves strong *F1-Score** for fraud despite class imbalance. I'll continue to tune model parameters, use various data transform datasets, etc., in order to achieve even better precision-recall trade-off.\n",
    "- The dataset used--baseline_dummy_encode_transacts\n",
    "   - consisting of the baseline (non-scaled/log1p) columns and dummy encodings of type column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bc18926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data pre-processing complete: SMOTE applied and features scaled.\n",
      "üìä Class balance after SMOTE:\n",
      " isFraud\n",
      "0    119844\n",
      "1    119844\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SMOTE generates synthetic minority class (fraud) samples\n",
    "sm = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "# Step 5: Scale features (mean=0, variance=1)\n",
    "# Fit scaler only on training data to prevent data leakage/peeking\n",
    "scaler = StandardScaler()\n",
    "X_resampled_scaled = scaler.fit_transform(X_resampled)\n",
    "X_test_scaled = scaler.transform(X_test)  # Apply same scaler to test set\n",
    "\n",
    "print(\"‚úÖ Data pre-processing complete: SMOTE applied and features scaled.\")\n",
    "print(\"üìä Class balance after SMOTE:\\n\", y_resampled.value_counts())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b6fa586f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix \n",
      " [[29961     0]\n",
      " [   25    14]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     29961\n",
      "           1       1.00      0.36      0.53        39\n",
      "\n",
      "    accuracy                           1.00     30000\n",
      "   macro avg       1.00      0.68      0.76     30000\n",
      "weighted avg       1.00      1.00      1.00     30000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    " #initialize a Support Vector Classifier with RBF kernel to handle non-linearity\n",
    "svc_non_linear = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "\n",
    "# Train the classifier on the XOR dataset\n",
    "svc_non_linear.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the same dataset\n",
    "yhat = svc_non_linear.predict(X_test)\n",
    "\n",
    "confusion = confusion_matrix(y_test, yhat)\n",
    "class_report = classification_report(y_test, yhat)\n",
    "\n",
    "print(\"Confusion Matrix \\n\", confusion)\n",
    "print(\"\\nClassification Report:\\n\", class_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement random search on the LinearSVC model to find best hyperparams\n",
    "svc_params_grid = {\n",
    "    'C': np.logspace(-1, 2, 5),            \n",
    "    'kernel': ['rbf', 'sigmoid'],          \n",
    "    'gamma': ['scale', 'auto'],\n",
    "    #'degree': [2, 3, 4, 5] \n",
    "}\n",
    "\n",
    "svc = SVC(max_iter=10000, random_state=42)\n",
    "\n",
    "# set up RandomizedSearchCV with 5-fold cross-validation\n",
    "random_search = RandomizedSearchCV(svc, param_distributions=svc_params_grid, scoring='f1', n_jobs=-1, cv=5, random_state=42)\n",
    "\n",
    "# fit this model on your training data\n",
    "random_search.fit(X_train, y_train)  # ran for 36m 12.0s without n_jobs param being set and took 16m 13.6s when scaling before training and using all 8 cores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1a36313e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix \n",
      " [[39948     0]\n",
      " [   44     8]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     39948\n",
      "           1       1.00      0.15      0.27        52\n",
      "\n",
      "    accuracy                           1.00     40000\n",
      "   macro avg       1.00      0.58      0.63     40000\n",
      "weighted avg       1.00      1.00      1.00     40000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_svc = random_search.best_estimator_\n",
    "\n",
    "# make predictions on the same dataset\n",
    "yhat = best_svc.predict(X_test)\n",
    "\n",
    "confusion = confusion_matrix(y_test, yhat)\n",
    "class_report = classification_report(y_test, yhat)\n",
    "\n",
    "print(\"Confusion Matrix \\n\", confusion)\n",
    "print(\"\\nClassification Report:\\n\", class_report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
